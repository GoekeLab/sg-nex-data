{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoekeLab/sg-nex-data/blob/master/docs/colab/Introduction_genomics_3_GoogleColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Genomics Workshop 3: A long read RNA-Seq pipeline in Nextflow\n",
        "\n",
        "Bioinformatics pipelines often consist of multiple tools that are used to generate the final output. In this workshop we will use a workflow manager (Nextflow) to automatically execute the long read RNA-Seq workflow. We will be using long read Nanopore RNA-Seq data from the Singapore Nanpore Expression Project (SG-NEx).\n",
        "\n",
        "\n",
        "### Using Google Colab\n",
        "\n",
        "This tutorial requires access to a shell (i.e. Linux, MacOS, or the Windows Subsystem for Linux/WSL). If you do not have access to any shell, you can run this tutorial on Google Colab by clicking the badge on top.\n",
        "\n",
        "If you use Google Colab, you have to add `!` before any shell command to execute it in a subshell. Changing working directories requires to add `%` instead, which executes the command globally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We will use the AWS command line interface to access and download the SG-NEx data. We will use minimap2 for read alignment, samtools for sam to bam file conversion, and Bambu for quantification and transcript discovery. We will use Nextflow to run the workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Google Colab\n",
        "\n",
        "This tutorial requires access to a shell (i.e. Linux, MacOS, or the Windows Subsystem for Linux/WSL). When using Google Colab, you have to add ! before any shell command to execute it in a subshell. Changing working directories requires to add % instead, which executes the command globally.\n",
        "\n",
        "This script enables the execution of R commands from Google Colab (using the Python template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUXOa9dtebaC"
      },
      "outputs": [],
      "source": [
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we will create a folder `software`, which will be used to download software that we want to install:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S3Indp-g75R"
      },
      "outputs": [],
      "source": [
        "! mkdir software \n",
        "%cd software\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Nextflow** is a workflow management system that enables users to write reproducible and portable pipelines. Nextflow provides many features that can be very helpful when developing complex pipelines, such as the ability to restart a disrupted workflow run, and the generation of a workflow execution report. You can read more about Nextflow here: https://www.nextflow.io/. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvswMuqCrUpg"
      },
      "outputs": [],
      "source": [
        "! curl -s https://get.nextflow.io | bash\n",
        "! sudo ln -s /content/software/nextflow /usr/bin/nextflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The **AWS Command Line Interface** can be used to access data stored on the AWS cloud S3 objects.\n",
        "\n",
        "You can install `awscli` using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKlyWWLYrXS9"
      },
      "outputs": [],
      "source": [
        "! python -m pip install awscli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Minimap2** is a software to align the sequencing reads (fastq files) to a reference genome. Here we will use pre-compiled binaries, for detailed installation instructions you can refer to the [Minimap2 website] (https://github.com/lh3/minimap2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJPD90MJeiFo"
      },
      "outputs": [],
      "source": [
        "! curl -L https://github.com/lh3/minimap2/releases/download/v2.26/minimap2-2.26_x64-linux.tar.bz2 | tar -jxvf -\n",
        "! sudo ln -s /content/software/minimap2-2.26_x64-linux/minimap2 /usr/bin/minimap2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reads which are aligned to a reference genome (for example using Minimap2) are stored in sam files (or bam files, which are compressed sam files). **Samtools** is a collection of tools to handle sam and bam files. You can install the latest version of Samtools as described online (http://www.htslib.org/download/). Depending on the operating system, you can also install Samtools using the following command (which might not be the latest version):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeGdM-haenry"
      },
      "outputs": [],
      "source": [
        "! sudo apt install samtools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Bambu** is a R package which requires a recent version of R (>4.0). Installation guidelines can be found online: <https://www.r-project.org/>\n",
        "\n",
        "R is already installed on Google Colab. [Bambu](https://github.com/GoekeLab/bambu) can be installed either through Github or through Bioconductor (recommended). This step might take 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n79n8SxRtH7A"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "if (!require(\"BiocManager\", quietly = TRUE))\n",
        "    install.packages(\"BiocManager\")\n",
        "\n",
        "BiocManager::install(\"bambu\", update=FALSE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After all software is installed, we change back to the parent directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDo_fOk2pmq6"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Download \n",
        "\n",
        "For this workshop, we will create a `workshop` directory and three sub-directories (`reference`, `fastq`, and `nextflow`) that will store the human genome sequence and annotations (reference), the sequencing data reads(fastq), and the nextflow script that describes the complete workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgegnP4wlr_q"
      },
      "outputs": [],
      "source": [
        "! mkdir -p workshop/reference\n",
        "! mkdir workshop/fastq\n",
        "! mkdir workshop/nextflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Singapore Nanopore Expression Project (SG-NEx) has generated a comprehensive resource of long read RNA-Sequencing data using the Oxford Nanopore Sequencing third generation sequencing platform. The data is hosted on the [AWS Open Data Registry](https://registry.opendata.aws/sgnex/) and described in detail here: <https://github.com/GoekeLab/sg-nex-data>\n",
        "\n",
        "**Downloading the human genome sequence and annotations (fa, fa.fai, and gtf)**\n",
        "\n",
        "For this workshop we will be using a reduced data set which only includes data from the human chromosome 22. The data can be accessed using the AWS command line interface (or using direct links, which you can find in the online documentation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwX9CKLbfLja"
      },
      "outputs": [],
      "source": [
        "! aws s3 cp --no-sign-request s3://sg-nex-data/data/data_tutorial/annotations/hg38_chr22.fa workshop/reference/\n",
        "! aws s3 cp --no-sign-request s3://sg-nex-data/data/data_tutorial/annotations/hg38_chr22.fa.fai workshop/reference/\n",
        "! aws s3 cp --no-sign-request s3://sg-nex-data/data/data_tutorial/annotations/hg38_chr22.gtf workshop/reference/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Downloading the sequencing reads**\n",
        "\n",
        "Here we will use the fastq files from the SG-NEx project that contain reads from chromosome 22:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iviHoD_wfOum"
      },
      "outputs": [],
      "source": [
        "! aws s3 sync --no-sign-request s3://sg-nex-data/data/data_tutorial/fastq/ workshop/fastq/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Workflow execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Here we will run the workflow that combines workshop 1 (read alignment and sam to bam conversion) with workshop 2 (transcript discovery and quantification with Bambu). \n",
        "\n",
        "The workflow will be executed using Nextflow (please refer to the lecture slides for additional details). Workflow results are cached in a directory `$PWD/work`, where `$PWD` is the path to the current directory. Here we will execute the workflow from the workshop directory:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following command downloads the nextflow script into the `nextflow/` directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgrosp6il650"
      },
      "outputs": [],
      "source": [
        "! wget \"https://raw.githubusercontent.com/GoekeLab/sg-nex-data/master/docs/colab/workflow_longReadRNASeq.nf\"  -P nextflow/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj9u9JOwgZJx"
      },
      "outputs": [],
      "source": [
        "! ls nextflow/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run the workflow we use the nextflow command with the option `-with-report` to provide a summary report about the workflow run, and the option `-resume` that allows us to resume a run with existing intermediate results if it was disrupted or modified. The other arguments are defined in the workflow script, and provide the path to the reads, reference genome files, and the output directory:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2PlfKTjgzcc"
      },
      "outputs": [],
      "source": [
        "! nextflow run nextflow/workflow_longReadRNASeq.nf -with-report -resume \\\n",
        "      --reads $PWD/fastq/A549_directRNA_sample2.fastq.gz \\\n",
        "      --refFa $PWD/reference/hg38_chr22.fa \\\n",
        "      --refGtf $PWD/reference/hg38_chr22.gtf \\\n",
        "      --outdir $PWD/results/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the run is complete, you can list all results that are generated and stored in the output directory using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vklt2KksmHiK"
      },
      "outputs": [],
      "source": [
        "! ls -lh results/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see the results from transcript discovery (stored in the *.gtf file), and the results from transcript and gene expression quantification. \n",
        "\n",
        "With the following command you can view the read count for some of the transcripts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! head results/counts_transcript.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The file shows the transcript id, the corresponding gene id, and the number of aligned reads. The prefix \"Bambu\" indicates that a gene or transcript is newly discovered, prefix \"ENS\" corresponds to gene and transcript IDs from the annotations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cache and resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nextflow stores results from the workflow execution for each process in the `$PWD/work` directories, which allows us to modify parts of the workflow and resume the run without recomputing results from processes which were not changed. \n",
        "\n",
        "Here we will change the transcript discovery argument in bambu to NDR=0 (no transcript discovery). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! cp nextflow/workflow_longReadRNASeq.nf nextflow/workflow_longReadRNASeq_original.nf\n",
        "! sed -i 's/'NDR=1'/'NDR=0'/g' nextflow/workflow_longReadRNASeq.nf \n",
        "! diff nextflow/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This change will modify the Bambu process, but not the process for alignment or sam to bam conversion. Using the `-resume` option, we can now execute the workflow using the cached results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! nextflow run nextflow/workflow_longReadRNASeq.nf -with-report -resume \\\n",
        "      --reads $PWD/fastq/A549_directRNA_sample2.fastq.gz \\\n",
        "      --refFa $PWD/reference/hg38_chr22.fa \\\n",
        "      --refGtf $PWD/reference/hg38_chr22.gtf \\\n",
        "      --outdir $PWD/results/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results directory now includes results from the modified Bambu process, where only annotated transcripts and genes will be quantified:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! head results/counts_transcript.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean working directories when the run is complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Once the run is completed and the results are obtained, the work directories should be cleaned. This can be done manually, or using the `nextflow clean` command. First, we will list the nextflow runs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! nextflow log -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now either specify to remove cached data from a specific run, or we can delete the data from the last run (default option). the `-n` argument indicated a dry-run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! nextflow clean -n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the `-f` argument, the files will be removed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! nextflow clean -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can again list the nextflow runs after this clean step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! nextflow log -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can repeat the steps to list files that will be deleted when we run nextflow clean (`-n` option for dryrun), and finally delete these files (`-f`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! nextflow clean -n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! nextflow clean -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 1.3.4. The execution report\n",
        "\n",
        "The `-with-report` option generates an execution report that contains useful information about resources that were used by each process. The report is stored in the execution directory as a html file.\n",
        "\n",
        ">**Exercise:** Download and view the reports that were generated. How many processes were executed? How many CPUs were used by them, and how much memory? Which process took the longest to complete?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMlfODH3W+SfHeUvHzRzKkk",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Introduction_genomics_workflow.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
